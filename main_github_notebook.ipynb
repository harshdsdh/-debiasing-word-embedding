{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch import cuda\n",
    "from torch.autograd import Variable\n",
    "from gensim.models import KeyedVectors\n",
    "from math import ceil\n",
    "from hyperparams_glove import Hyperparams as hp\n",
    "#from hyperparams_gn_glove import Hyperparams as hp\n",
    "import random\n",
    "import pre_train_autoencoder\n",
    "import pre_train_classifier\n",
    "import optim\n",
    "import model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "liveloss = PlotLosses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_train_autoencoder import pre_train_autoencoder\n",
    "from pre_train_classifier import pre_train_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperparams_glove import Hyperparams as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embedding\n"
     ]
    }
   ],
   "source": [
    "print('loading word embedding')\n",
    "#binary is false as data is in text format\n",
    "emb = KeyedVectors.load_word2vec_format(hp.word_embedding, binary=hp.emb_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words_not_in_word2emb(emb,words):\n",
    "    return [word for word in words if word in emb]\n",
    "def make_no_gender_words(f,emb):\n",
    "    return remove_words_not_in_word2emb(emb,[l.strip() for l in f])\n",
    "\n",
    "def remove_pairs_not_in_word2emb(emb,pairs):\n",
    "    return [word1 for word1,word2 in pairs if word1 in emb and word2 in emb],\\\n",
    "        [word2 for word1,word2 in pairs if word2 in emb and word1 in emb]\n",
    "def make_G_pair_words(f,emb):\n",
    "    df = pd.read_csv(f)\n",
    "    female_words = df.iloc[:,0].values.tolist()\n",
    "    male_words = df.iloc[:,1].values.tolist()\n",
    "    \n",
    "    if len(female_words)==len(male_words):\n",
    "        return remove_pairs_not_in_word2emb(emb,[[word1,word2] for word1,word2 in zip(female_words,male_words)])\n",
    "    elif len(female_words)!=len(male_words):\n",
    "        return remove_words_not_in_word2emb(emb,female_words), remove_words_not_in_word2emb(emb, male_words)\n",
    "def make_pair_words(f,emb):\n",
    "    df = pd.read_csv(f, sep='\\t')\n",
    "    female_words = df.iloc[:,0].values.tolist()\n",
    "    male_words = df.iloc[:,1].values.tolist()\n",
    "    \n",
    "    if len(female_words)==len(male_words):\n",
    "        return remove_pairs_not_in_word2emb(emb,[[word1,word2] for word1,word2 in zip(female_words,male_words)])\n",
    "    elif len(female_words)!=len(male_words):\n",
    "        return remove_words_not_in_word2emb(emb,female_words), remove_words_not_in_word2emb(emb, male_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(words, tags=None, limit_size=None, sampling=None):\n",
    "    perm = torch.randperm(len(words))\n",
    "    words = [words[idx.item()] for idx in perm]\n",
    "    return words, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data1(words, p_pos):\n",
    "    p_pos = list(p_pos)\n",
    "    perm = np.random.choice(list(range(len(words))),size=len(words),p=p_pos)\n",
    "    sorted_inds = np.sort(perm)\n",
    "    words = [words[idx.item()] for idx in sorted_inds]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dev(gender_words,no_gender_words,stereotype_words):\n",
    "    stereotype_words=stereotype_words['female']+stereotype_words['male']\n",
    "    gender_pairs = [[female,male] for female,male in zip(gender_words['female'], gender_words['male'])]\n",
    "    \n",
    "    no_gender_words,_ = shuffle_data(no_gender_words)\n",
    "    gender_pairs,_ = shuffle_data(gender_pairs)\n",
    "    stereotype_words,_ = shuffle_data(stereotype_words)\n",
    "    train_words={}\n",
    "    train_words['no_gender'] = no_gender_words[:-hp.dev_num]\n",
    "    train_words['female&male'] = gender_pairs[:-hp.dev_num]\n",
    "    train_words['stereotype'] = stereotype_words[:-hp.dev_num]\n",
    "    \n",
    "    dev_words={}\n",
    "    dev_words['no_gender'] = no_gender_words[-hp.dev_num:]\n",
    "    dev_words['female&male'] = gender_pairs[-hp.dev_num:]\n",
    "    dev_words['stereotype'] = stereotype_words[-hp.dev_num:]\n",
    "    return train_words,dev_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optim(model, optimizer, learning_rate, lr_decay, max_grad_norm):\n",
    "    model_optim = optim.Optim(optimizer, learning_rate, lr_decay, max_grad_norm)\n",
    "    model_optim.set_parameters(model.parameters())\n",
    "    return model_optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n"
     ]
    }
   ],
   "source": [
    "print(\"loading data\")\n",
    "stereotype_words={}\n",
    "gender_words={}\n",
    "#making sure that our no gender words are present in embedding\n",
    "no_gender_words=make_no_gender_words(open(hp.no_gender_words), emb)\n",
    "stereotype_words['female'],stereotype_words['male'] = make_pair_words(hp.stereotype_words,emb)\n",
    "gender_words['female'],gender_words['male']=make_G_pair_words(hp.gender_words,emb )\n",
    "all_words =no_gender_words+stereotype_words['female']+stereotype_words['male']+gender_words['female']+gender_words['male']\n",
    "train_words,dev_words = create_train_dev(gender_words, no_gender_words,stereotype_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2emb={}\n",
    "for word in all_words:\n",
    "    word2emb[word] = emb[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abandonment', 'abate', 'aberrant', 'abiding', 'able', 'abolition', 'abomination', 'abrupt', 'absorbing', 'absorption', 'abstention', 'abstraction', 'absurd', 'absurdity', 'abundance', 'abundantly', 'accept', 'acceptable', 'access', 'accident', 'accidentally', 'accompany', 'accomplish', 'according', 'accordingly', 'account', 'accumulation', 'accurate', 'accuse', 'achieve', 'achievement', 'acid', 'acknowledge', 'acquire', 'actuality', 'adaptable', 'adaptation', 'addictive', 'adherence', 'adjacent', 'adjustable', 'adjustment', 'adjustments', 'admittedly', 'ado', 'adorable', 'adore', 'adorn', 'advancement', 'advent', 'adverse', 'adversity', 'advertisement', 'aerial', 'afar', 'affected', 'afternoon', 'ago', 'agonies', 'agree', 'agreement', 'agricultural', 'air', 'aircraft', 'airliner', 'airport', 'alarms', 'alongside', 'aloof', 'alternately', 'amazing', 'amazingly', 'amount', 'amusement', 'analogous', 'analyses', 'answer', 'apartments', 'apparatus', 'apparent', 'apparently', 'appetizer', 'apple', 'appoint', 'appointment', 'appreciate', 'approach', 'appropriate', 'approval', 'approve', 'approximately', 'argument', 'arms', 'arrests', 'art', 'ask', 'astray', 'attack', 'attempt', 'attention', 'attraction', 'attributes', 'authority', 'aware', 'back', 'backdrops', 'bad', 'balance', 'ballots', 'banana', 'bar', 'barely', 'barrel', 'base', 'battery', 'be', 'become', 'bed', 'before', 'beforehand', 'begin', 'beginning', 'behavior', 'behind', 'being', 'belief', 'believe', 'bell', 'belong', 'below', 'benches', 'benefits', 'best', 'better', 'big', 'bigger', 'biggest', 'billion', 'bit', 'bite', 'blackboard', 'blast', 'blizzard', 'blood', 'blouse', 'blow', 'blue', 'blues', 'board', 'boat', 'body', 'bomb', 'bombing', 'bond', 'bone', 'boon', 'bother', 'bottle', 'bottles', 'boxes', 'branch', 'brass', 'bread', 'breath', 'breeze', 'bright', 'brighter', 'building', 'bulletin', 'burn', 'burst', 'bus', 'buses', 'butter', 'bygone', 'calamity', 'calm', 'calmly', 'camera', 'camp', 'campaign', 'campus', 'can', 'cancer', 'candidate', 'canoe', 'canvas', 'cap', 'capability', 'capable', 'capacity', 'capital', 'capsule', 'captain', 'capture', 'car', 'carbon', 'card', 'cardboard', 'care', 'career', 'careful', 'carefully', 'carpet', 'carrier', 'carry', 'case', 'cash', 'cassette', 'cast', 'cataract', 'categorical', 'cathedral', 'cause', 'cell', 'center', 'centigrade', 'central', 'cerebellum', 'ceremonial', 'certain', 'certify', 'chair', 'chalk', 'chance', 'change', 'changing', 'channel', 'chapter', 'character', 'characteristic', 'characterize', 'charge', 'cheap', 'cheaper', 'cheapest', 'cheerful', 'cheerfully', 'chestnut', 'chin', 'china', 'choices', 'cinema', 'circuit', 'circulate', 'cities', 'city', 'clarify', 'clearance', 'clock', 'clumsy', 'coal', 'cocoon', 'coffee', 'coincide', 'cold', 'colder', 'color', 'come', 'comfort', 'comfortable', 'common', 'communicate', 'communication', 'community', 'compacts', 'company', 'comparison', 'competition', 'completion', 'compounds', 'condition', 'conference', 'confidence', 'confident', 'connection', 'consistent', 'control', 'conventional', 'conversation', 'coolest', 'copy', 'cork', 'corns', 'couch', 'cough', 'could', 'country', 'cover', 'crack', 'credit', 'creep', 'crime', 'crush', 'current', 'curve', 'daily', 'damage', 'damper', 'danger', 'dark', 'darkest', 'day', 'death', 'debate', 'debt', 'decade', 'decide', 'decided', 'decision', 'decrease', 'decreased', 'decreasing', 'deep', 'deeper', 'degree', 'denials', 'describe', 'described', 'describing', 'design', 'desire', 'destruction', 'detail', 'development', 'develops', 'different', 'digestion', 'direction', 'discover', 'discovering', 'discussion', 'disease', 'disgrace', 'disgust', 'disorder', 'display', 'dispute', 'distance', 'distant', 'distaste', 'distasteful', 'distinct', 'distinction', 'distinguish', 'distribute', 'distribution', 'district', 'diverse', 'diversity', 'divide', 'division', 'divorce', 'do', 'dodge', 'does', 'dollar', 'done', 'door', 'double', 'doubt', 'down', 'dream', 'dreams', 'dust', 'eagerly', 'ear', 'early', 'earn', 'earnings', 'earth', 'ease', 'easier', 'easily', 'east', 'eastern', 'easy', 'eat', 'eats', 'economic', 'edge', 'education', 'educations', 'effect', 'egg', 'end', 'enforcement', 'engage', 'english', 'enhance', 'enhanced', 'enhances', 'enhancing', 'equator', 'error', 'essentially', 'establish', 'establishment', 'estate', 'estimate', 'ethical', 'europe', 'evaporate', 'event', 'evidence', 'example', 'exchange', 'existence', 'expansion', 'experience', 'eye', 'eyelids', 'eyes', 'fact', 'fall', 'fast', 'faster', 'fear', 'fed', 'federal', 'fee', 'feed', 'feel', 'feeling', 'fellow', 'fiction', 'field', 'find', 'finds', 'finger', 'fire', 'flame', 'flashlight', 'flight', 'fly', 'flying', 'fold', 'food', 'force', 'form', 'free', 'freely', 'french', 'front', 'fruit', 'full', 'furniture', 'garbage', 'garlic', 'generate', 'generating', 'get', 'gets', 'give', 'glass', 'globe', 'go', 'goals', 'goes', 'going', 'gold', 'good', 'got', 'government', 'grain', 'grammar', 'grapefruit', 'grass', 'great', 'greater', 'greatest', 'green', 'grip', 'group', 'growth', 'half', 'hall', 'hand', 'handful', 'handle', 'hands', 'hang', 'hanger', 'happen', 'happily', 'happy', 'harbor', 'hard', 'harder', 'hardly', 'harmony', 'hate', 'have', 'head', 'heap', 'hear', 'hearing', 'heat', 'heavy', 'help', 'helps', 'hid', 'hidden', 'hill', 'history', 'hole', 'hope', 'hotel', 'hottest', 'hour', 'ice', 'idea', 'ideas', 'idiom', 'implement', 'importance', 'important', 'impose', 'impossible', 'impress', 'impression', 'impressions', 'impressive', 'improve', 'improvement', 'impulse', 'inconsistent', 'increase', 'increases', 'increasing', 'increasingly', 'incredible', 'indeed', 'industry', 'informative', 'ink', 'insect', 'insight', 'instinct', 'instrument', 'insurance', 'interest', 'international', 'invention', 'iron', 'is', 'issues', 'its', 'jar', 'join', 'jump', 'junction', 'keenly', 'keep', 'kept', 'killing', 'kit', 'knew', 'know', 'knowledge', 'known', 'label', 'ladders', 'land', 'language', 'largest', 'late', 'latent', 'laugh', 'lead', 'learning', 'leather', 'leave', 'leaves', 'leg', 'lemon', 'length', 'let', 'letter', 'level', 'lift', 'light', 'like', 'likes', 'limit', 'liquid', 'list', 'listen', 'listened', 'listening', 'listens', 'lists', 'live', 'load', 'local', 'london', 'long', 'longer', 'longest', 'look', 'looked', 'looking', 'loss', 'lounge', 'low', 'lower', 'lows', 'luck', 'luckiest', 'lucky', 'main', 'mainly', 'mains', 'mainstream', 'maintain', 'maintenance', 'major', 'majority', 'make', 'maker', 'many', 'margin', 'mark', 'may', 'meal', 'mean', 'measure', 'meat', 'meeting', 'memory', 'metal', 'middle', 'might', 'millionth', 'mind', 'minority', 'minute', 'miracle', 'mist', 'money', 'month', 'morning', 'motion', 'mountain', 'mouth', 'move', 'moved', 'mucus', 'multiple', 'museum', 'music', 'myriad', 'name', 'narrower', 'nation', 'national', 'necessarily', 'necessary', 'neck', 'need', 'net', 'new', 'newer', 'newly', 'news', 'nice', 'night', 'nightly', 'noise', 'noisier', 'noisiest', 'none', 'nonetheless', 'nor', 'normal', 'normally', 'nose', 'note', 'number', 'oases', 'observation', 'offer', 'oil', 'old', 'older', 'oldest', 'onion', 'onions', 'only', 'operation', 'opinion', 'order', 'organization', 'other', 'overflow', 'page', 'pain', 'paint', 'paper', 'part', 'particular', 'paste', 'payment', 'pen', 'pencil', 'pending', 'pepper', 'perfect', 'perfectly', 'perform', 'phenomena', 'phone', 'photo', 'pitches', 'place', 'play', 'pleasure', 'plenty', 'point', 'poison', 'pole', 'polish', 'pool', 'porter', 'position', 'possible', 'powder', 'power', 'predicts', 'price', 'print', 'problems', 'process', 'produce', 'productive', 'profit', 'property', 'prose', 'protest', 'pull', 'punishment', 'pupils', 'purpose', 'push', 'put', 'quality', 'question', 'quick', 'quicker', 'quickly', 'quieting', 'rain', 'rainfalls', 'range', 'rank', 'rapid', 'rapidly', 'rare', 'rarely', 'rate', 'ratios', 'raw', 'ray', 'reach', 'react', 'reaction', 'reading', 'real', 'reason', 'recent', 'recliner', 'recognize', 'recommend', 'recommendation', 'record', 'reflect', 'reflection', 'regret', 'relation', 'religion', 'remind', 'rent', 'reportedly', 'representative', 'request', 'resemblance', 'respect', 'rest', 'result', 'retain', 'reward', 'rhythm', 'rhythms', 'rice', 'right', 'river', 'road', 'roll', 'roof', 'room', 'rub', 'rule', 'run', 'running', 'safe', 'safely', 'safer', 'salt', 'sand', 'saw', 'say', 'says', 'scale', 'schedule', 'scheme', 'screamed', 'screaming', 'sea', 'seat', 'second', 'secretary', 'see', 'seeing', 'seem', 'seemingly', 'sees', 'selection', 'self', 'sense', 'sentence', 'serious', 'seriously', 'servant', 'seventh', 'several', 'shade', 'shake', 'shelf', 'shirt', 'shock', 'shoes', 'should', 'show', 'shuffle', 'shuffles', 'sick', 'side', 'siege', 'sign', 'silk', 'silver', 'simplest', 'size', 'skill', 'sky', 'sleep', 'sleeping', 'slept', 'slip', 'slope', 'slow', 'slower', 'slowing', 'slowly', 'smarter', 'smartest', 'smash', 'smell', 'smile', 'smoke', 'smoothing', 'sneeze', 'snow', 'society', 'some', 'song', 'sort', 'sound', 'space', 'speak', 'speaks', 'special', 'spending', 'stacks', 'stage', 'stanford', 'start', 'statement', 'steam', 'steel', 'step', 'stitch', 'stone', 'stones', 'stop', 'story', 'stress', 'stretch', 'stronger', 'structure', 'substance', 'sufficiently', 'sugar', 'suggestion', 'summer', 'support', 'surprise', 'swift', 'swiftly', 'swim', 'system', 'systems', 'take', 'talk', 'taste', 'tasteful', 'tax', 'teeth', 'tell', 'ten', 'tendency', 'test', 'text', 'then', 'theories', 'theory', 'thing', 'things', 'think', 'thinks', 'thought', 'thousand', 'thousandth', 'thump', 'thunder', 'time', 'tin', 'tissue', 'title', 'tokyo', 'tomatoes', 'tongue', 'took', 'top', 'touch', 'tougher', 'town', 'toy', 'trade', 'transfer', 'transport', 'tree', 'trick', 'trouble', 'try', 'turn', 'twist', 'unacceptable', 'unaware', 'uncomfortable', 'undecided', 'underside', 'unexpectedly', 'unfortunately', 'unimpressive', 'uninformative', 'unit', 'university', 'unknown', 'unproductive', 'use', 'useful', 'utterly', 'value', 'vanishes', 'variously', 'verse', 'very', 'vessel', 'view', 'visibility', 'visit', 'voice', 'walk', 'want', 'ware', 'warmer', 'was', 'wash', 'waste', 'water', 'watery', 'wave', 'way', 'weakest', 'weather', 'week', 'weekend', 'weeks', 'welfare', 'went', 'westward', 'whichever', 'whole', 'widest', 'will', 'wind', 'wine', 'winter', 'wire', 'wishes', 'wood', 'wool', 'word', 'words', 'world', 'worse', 'worst', 'would', 'wound', 'writing', 'yacht', 'year', 'yearly', 'yen', 'young', 'younger', 'youngest', 'zero', 'zigzag', 'zone', 'apparel', 'assistant', 'baker', 'bathing', 'beautiful', 'beauty', 'blonde', 'bookkeeper', 'ca', 'cashier', 'chatty', 'cheerleader', 'cheerleading', 'clerk', 'cocktail', 'cooking', 'counselor', 'crafting', 'cute', 'dancer', 'educator', 'emotional', 'flirt', 'flirtatious', 'flower', 'gossip', 'graceful', 'hairdresser', 'hairdryer', 'homemaker', 'hooker', 'housekeeper', 'housekeepers', 'housework', 'hula', 'indoor', 'jealousy', 'jewelry', 'kawaii', 'laundering', 'librarian', 'librarians', 'lotion', 'lovely', 'marvelous', 'mirror', 'moisturizer', 'nanny', 'neat', 'nurse', 'nursery', 'nurses', 'nurturing', 'parenting', 'passive', 'pink', 'pretty', 'receptionist', 'ribbon', 'romance', 'romantic', 'secretary', 'selfie', 'server', 'sew', 'sewing', 'shopping', 'smoothie', 'soft', 'softball', 'stylist', 'submissive', 'sweet', 'tailor', 'tall', 'teacher', 'thin', 'violinist', 'waiter', 'weak', 'yoga', 'hysterical', 'makeup', 'aggressive', 'tycoon', 'warrior', 'ambitious', 'trucker', 'welder', 'strong', 'terrorist', 'soldier', 'astronomer', 'sniper', 'skipper', 'banker', 'baseball', 'sergeant', 'bodyguard', 'boss', 'boxer', 'cabbie', 'captain', 'cardiologist', 'carpenter', 'ceo', 'chairperson', 'chancellor', 'chef', 'colonel', 'commander', 'conductor', 'police', 'custodian', 'dentist', 'detective', 'diplomat', 'doctor', 'driving', 'drummer', 'economist', 'electrician', 'engineer', 'engineering', 'entrepreneur', 'financier', 'firefighter', 'footballer', 'gambler', 'gamer', 'gangster', 'geek', 'geeks', 'gentle', 'guitarist', 'industrialist', 'inventor', 'investigator', 'laborer', 'lawyer', 'leader', 'lieutenant', 'lifeguard', 'magistrate', 'manager', 'marshal', 'mathematician', 'mechanic', 'muscle', 'muscular', 'owner', 'philosopher', 'physicist', 'pilot', 'plumber', 'politician', 'president', 'professor', 'programmer', 'rugby', 'sailor', 'science', 'scientist', 'sculptor', 'blue', 'football', 'witches', 'maidservant', 'mothers', 'diva', 'actress', 'spinster', 'mama', 'duchesses', 'countrywomen', 'hostesses', 'clitoris', 'princess', 'governesses', 'abbess', 'women', 'widow', 'ladies', 'sorceresses', 'madam', 'brides', 'baroness', 'niece', 'widows', 'lady', 'sister', 'brides', 'nun', 'obstetrics', 'her', 'marchioness', 'princesses', 'empresses', 'mare', 'chairwoman', 'convent', 'priestesses', 'girlhood', 'ladies', 'queen', 'gals', 'mommies', 'maid', 'spokeswoman', 'seamstress', 'cowgirls', 'chick', 'spinsters', 'empress', 'mommy', 'gals', 'enchantress', 'gal', 'motherhood', 'estrogen', 'godmother', 'strongwoman', 'goddess', 'matriarch', 'aunt', 'sisterhood', 'hostess', 'estradiol', 'wife', 'mom', 'stewardess', 'females', 'viagra', 'spokeswomen', 'ma', 'belle', 'minx', 'maiden', 'witch', 'miss', 'nieces', 'mothered', 'cow', 'granddaughter', 'stepmothers', 'grandmothers', 'schoolgirl', 'hen', 'granddaughters', 'bachelorette', 'moms', 'her', 'mistress', 'lass', 'policewoman', 'nun', 'actresses', 'girlfriend', 'councilwoman', 'lady', 'stateswoman', 'maternal', 'lass', 'landlady', 'ladies', 'wenches', 'sorority', 'duchess', 'chicks', 'fiancee', 'fillies', 'wives', 'maternity', 'she', 'businesswoman', 'masseuses', 'heroine', 'doe', 'girlfriends', 'queens', 'sisters', 'mistresses', 'stepmother', 'brides', 'daughter', 'cowgirl', 'daughters', 'mezzo', 'saleswoman', 'mistress', 'nuns', 'headmistresses', 'lasses', 'congresswoman', 'priestess', 'abbesses', 'toque', 'sororities', 'stewardesses', 'filly', 'czarina', 'stepdaughters', 'herself', 'girls', 'lionesses', 'lady', 'vagina', 'hers', 'masseuse', 'cows', 'aunts', 'wench', 'toques', 'wife', 'lioness', 'sorceress', 'effeminate', 'mother', 'lesbians', 'female', 'waitresses', 'ovum', 'stepdaughter', 'businesswomen', 'heiress', 'waitress', 'headmistress', 'woman', 'governess', 'bride', 'grandma', 'bride', 'gal', 'lesbian', 'ladies', 'girl', 'grandmother', 'mare', 'maternity', 'hens', 'nuns', 'heroines', 'wizards', 'manservant', 'fathers', 'divo', 'actor', 'bachelor', 'papa', 'dukes', 'countrymen', 'hosts', 'penis', 'prince', 'governors', 'abbot', 'men', 'widower', 'gentlemen', 'sorcerers', 'sir', 'bridegrooms', 'baron', 'nephew', 'widowers', 'lord', 'brother', 'grooms', 'priest', 'andrology', 'his', 'marquis', 'princes', 'emperors', 'stallion', 'chairman', 'monastery', 'priests', 'boyhood', 'fellas', 'king', 'dudes', 'daddies', 'manservant', 'spokesman', 'tailor', 'cowboys', 'dude', 'bachelors', 'emperor', 'daddy', 'guys', 'enchanter', 'guy', 'fatherhood', 'androgen', 'godfather', 'strongman', 'god', 'patriarch', 'uncle', 'brotherhood', 'host', 'testosterone', 'husband', 'dad', 'steward', 'males', 'cialis', 'spokesmen', 'pa', 'beau', 'stud', 'bachelor', 'wizard', 'sir', 'nephews', 'fathered', 'bull', 'grandson', 'stepfathers', 'grandfathers', 'schoolboy', 'rooster', 'grandsons', 'bachelor', 'dads', 'him', 'master', 'lad', 'policeman', 'monk', 'actors', 'boyfriend', 'councilman', 'fella', 'statesman', 'paternal', 'chap', 'landlord', 'lords', 'blokes', 'fraternity', 'duke', 'dudes', 'fiance', 'colts', 'husbands', 'paternity', 'he', 'businessman', 'masseurs', 'hero', 'deer', 'boyfriends', 'kings', 'brothers', 'masters', 'stepfather', 'grooms', 'son', 'cowboy', 'sons', 'baritone', 'salesman', 'paramour', 'monks', 'headmasters', 'lads', 'congressman', 'priest', 'abbots', 'beard', 'fraternities', 'stewards', 'colt', 'czar', 'stepsons', 'himself', 'boys', 'lions', 'gentleman', 'penis', 'his', 'masseur', 'bulls', 'uncles', 'bloke', 'beards', 'hubby', 'lion', 'sorcerer', 'macho', 'father', 'gays', 'male', 'waiters', 'sperm', 'stepson', 'businessmen', 'heir', 'waiter', 'headmaster', 'man', 'governor', 'bridegroom', 'grandpa', 'groom', 'dude', 'gay', 'gents', 'boy', 'grandfather', 'gelding', 'paternity', 'roosters', 'priests', 'heros']\n"
     ]
    }
   ],
   "source": [
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "def pre_train_autoencoder(hp,encoder, encoder_optim,decoder, decoder_optim, train_words, dev_words, word2emb,shuffle_data1):\n",
    "\n",
    "    train_size  = len(train_words['no_gender']) + len(train_words['female&male'])+ len(train_words['stereotype'])\n",
    "    dev_size = len(dev_words['no_gender']) + len(dev_words['female&male']) +len(dev_words['stereotype'])\n",
    "    classifier_criterion = nn.BCELoss()\n",
    "    decoder_criterion = nn.MSELoss()\n",
    "    encoder1=encoder\n",
    "    decode2 =decoder\n",
    "    def vae_loss_function(mu, logsigma, kl_weight=0.0001):\n",
    "  \n",
    "        latent_loss = 0.5*torch.sum(logsigma.exp() + mu.pow(2)-1-logsigma)\n",
    "        vae_loss = kl_weight*latent_loss \n",
    "        #print(vae_loss)\n",
    "        return vae_loss\n",
    "    def sampling(z_mean,z_logsigma):\n",
    "    #print(hp.hid.shape)\n",
    "    #print(z_logsigma)\n",
    "        batch, latent_dim = z_mean.shape\n",
    "        #epsilon = torch.distributions.Normal(torch.tensor([0.0]), torch.tensor([1.0]),sample_shape=(batch,latent_dim))\n",
    "        epsilon = torch.rand((batch, latent_dim))\n",
    "\n",
    "        z = z_mean + torch.exp(0.5*z_logsigma)*epsilon\n",
    "        #print(z.shape)\n",
    "        return z\n",
    "    def encode(x):\n",
    "        #encoder.zero_grad()\n",
    "        \n",
    "        for i,words in enumerate(x):\n",
    "            emb1 =  torch.stack([torch.from_numpy(word2emb[word]) for word in words])\n",
    "            encoder_output = encoder(emb1)\n",
    "        z_mean = encoder_output[:,:hp.hidden_size//2]\n",
    "       \n",
    "        z_logsigma = encoder_output[:,hp.hidden_size//2:]\n",
    "        \n",
    "        return encoder_output, z_mean, z_logsigma\n",
    "    def encoder_1(emb_1):\n",
    "        encoder_output = encoder(emb_1)\n",
    "        z_mean = encoder_output[:,:hp.hidden_size//2]\n",
    "       \n",
    "        z_logsigma = encoder_output[:,hp.hidden_size//2:]\n",
    "        \n",
    "        return encoder_output, z_mean, z_logsigma\n",
    "    \n",
    "    def get_latent_mu(words,batch_size=4):\n",
    "        \n",
    "        N = len(words)\n",
    "        mu = np.zeros((N, hp.hidden_size//2))\n",
    "        for start_ind in range(0, N, batch_size):\n",
    "            end_ind = min(start_ind+batch_size, N+1)\n",
    "            if end_ind%batch_size!=0:\n",
    "                break\n",
    "            batch = (words[start_ind:end_ind])\n",
    "            _,batch_mu, _ = encode(batch)\n",
    "            \n",
    "            mu[start_ind:end_ind] = batch_mu.detach().numpy()\n",
    "        return mu\n",
    "    def get_training_sample_probabilities(train_words,bins=4, smoothing_fac=0.005):\n",
    "        mu = get_latent_mu(train_words)\n",
    "        training_sample_p = np.zeros(mu.shape[0])\n",
    "        for i in range(hp.hidden_size//2):\n",
    "            latent_distribution = mu[:,i]\n",
    "            # generate a histogram of the latent distribution\n",
    "            hist_density, bin_edges =  np.histogram(latent_distribution, density=True, bins=bins)\n",
    "\n",
    "            # find which latent bin every data sample falls in \n",
    "            bin_edges[0] = -float('inf')\n",
    "            bin_edges[-1] = float('inf')\n",
    "\n",
    "            # TODO: call the digitize function to find which bins in the latent distribution \n",
    "            #    every data sample falls in to\n",
    "            # https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.digitize.html\n",
    "            bin_idx = np.digitize(latent_distribution, bin_edges) # TODO\n",
    "\n",
    "            # smooth the density function\n",
    "            hist_smoothed_density = hist_density + smoothing_fac\n",
    "            hist_smoothed_density = hist_smoothed_density / np.sum(hist_smoothed_density)\n",
    "\n",
    "            # invert the density function \n",
    "            p = 1.0/(hist_smoothed_density[bin_idx-1])\n",
    "\n",
    "            # TODO: normalize all probabilities\n",
    "            p = p/np.sum(p)\n",
    "\n",
    "            # TODO: update sampling probabilities by considering whether the newly\n",
    "            #     computed p is greater than the existing sampling probabilities.\n",
    "            training_sample_p = np.maximum(p, training_sample_p)\n",
    "        \n",
    "    # final normalization\n",
    "        training_sample_p /= np.sum(training_sample_p)\n",
    "\n",
    "        return training_sample_p\n",
    "    def run_model(words, mode,p_in):\n",
    "        #batch_size=4\n",
    "        if mode==\"train\":\n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "        elif model == 'eval':\n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "        total_loss = 0\n",
    "        total_num = 0\n",
    "        total_decoder_loss = 0\n",
    "        total_vae_loss = 0\n",
    "        vae_loss=0\n",
    "        if mode=='train':\n",
    "            limit_size =  max([len(words['no_gender']), len(words['female&male']),\n",
    "                               len(words['stereotype'])])\n",
    "            words['no gender'],_ = \\\n",
    "                shuffle_data(words['no_gender'],\n",
    "                             limit_size=limit_size,\n",
    "                             sampling = hp.sampling)\n",
    "            words['female & male'],_ = \\\n",
    "                shuffle_data(words['female&male'],\n",
    "                             limit_size=limit_size,\n",
    "                             sampling=hp.sampling)\n",
    "            words['stereotype'],_ = \\\n",
    "                shuffle_data(words['stereotype'],\n",
    "                             limit_size=limit_size,\n",
    "                             sampling=hp.sampling)\n",
    "        elif mode=='eval':\n",
    "            limit_size = hp.dev_num\n",
    "        \n",
    "        inputs = [[gender[0], gender[1], stereotype, no_gender] for gender, stereotype, no_gender in zip(\n",
    "            words['female&male'], words['stereotype'], words['no_gender'])]\n",
    "        if mode=='eval':\n",
    "            x=inputs\n",
    "        else:\n",
    "            x = shuffle_data1(inputs,p_in)\n",
    "        \n",
    "        data_size = len(x)*4\n",
    "        b_s = 4\n",
    "        for j in range(len(inputs)//4):\n",
    "            if mode==\"train\":\n",
    "                y=x[:32]\n",
    "            if mode=='eval':\n",
    "                #j=4\n",
    "                y=inputs\n",
    "            for i,words in enumerate(y):\n",
    "                #print(y)\n",
    "                #z_mean,z_logsigma,recon = call(batch)\n",
    "                #vae_loss_1  = debiasing_loss_function(z_mean,z_logsigma)\n",
    "                emb_dummy =  torch.stack([torch.from_numpy(word2emb[word]) for word in words])\n",
    "                encoder.zero_grad()\n",
    "                hidden, z_mean,z_logsigma = encoder_1(emb_dummy)\n",
    "                z = sampling(z_mean,z_logsigma)\n",
    "                decoder.zero_grad()\n",
    "                pre = decoder(z)\n",
    "                vae_loss = vae_loss_function(z_mean,z_logsigma)\n",
    "                decoder_loss = decoder_criterion(pre, emb_dummy)\n",
    "                #vae_loss *= (0.0005)\n",
    "                loss = decoder_loss+ vae_loss\n",
    "                #print(loss)\n",
    "                #total_vae_loss+=vae_loss_1.item()\n",
    "                if mode=='train':\n",
    "                    loss.backward()\n",
    "                    encoder_optim.step()\n",
    "                    decoder_optim.step()\n",
    "            total_loss += loss.item()\n",
    "            #total_num += len(y)\n",
    "        return total_loss/data_size\n",
    "                    \n",
    "\n",
    "    \n",
    "    print('start training')\n",
    "    best_loss = float('inf')\n",
    "    decoder_loss_list=[]\n",
    "    female_classifier_loss_list=[]\n",
    "    male_classifier_loss_list=[]\n",
    "    gender_stereotype_loss_list = []\n",
    "    gender_no_gender_loss_list = []\n",
    "    gender_vektor_loss_list = []\n",
    "    total_loss_list = []\n",
    "    female_classifier_acc_list = []\n",
    "    male_classifier_acc_list = []\n",
    "    vae_loss_list=[]\n",
    "    logs={}\n",
    "    inputs1 = [[gender[0], gender[1], stereotype, no_gender] for gender, stereotype, no_gender in zip(train_words['female&male'], train_words['stereotype'], train_words['no_gender'])]\n",
    "    p_inp = get_training_sample_probabilities(inputs1)\n",
    "        #l1=[]\n",
    "    min_loss = float('inf')\n",
    "    for epoch in range(300):\n",
    "        train_loss = run_model(train_words, mode='train',p_in=p_inp)\n",
    "        eval_loss = run_model(dev_words, mode='eval', p_in=p_inp)\n",
    "        print(epoch)\n",
    "    if eval_loss < min_loss:\n",
    "        min_epoch = epoch\n",
    "        min_loss = eval_loss\n",
    "        encoder_state_dict = encoder.state_dict()\n",
    "        decoder_state_dict = decoder.state_dict()\n",
    "        checkpoint = {\n",
    "            'encoder': encoder_state_dict,\n",
    "            'decoder': decoder_state_dict,\n",
    "            'hp': hp\n",
    "        }\n",
    "        torch.save(checkpoint,\n",
    "            '{}autoencoder_checkpoint'.format('./'))\n",
    "\n",
    "    checkpoint = torch.load('{}autoencoder_checkpoint'.format('./'))\n",
    "    torch.save(checkpoint, '{}autoencoder.pt'.format('./'))\n",
    "\n",
    "    import os\n",
    "    os.remove('{}autoencoder_checkpoint'.format('./'))\n",
    "\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training autoencoder\n",
      "Decoder(\n",
      "  (output_layer): Linear(in_features=150, out_features=300, bias=True)\n",
      "  (output_layer1): Linear(in_features=300, out_features=300, bias=True)\n",
      "  (tanh): Tanh()\n",
      ")\n",
      "start training\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n"
     ]
    }
   ],
   "source": [
    "if hp.pre_train_autoencoder:\n",
    "        print('Pre-training autoencoder')\n",
    "        encoder = model.Encoder(hp.emb_size, hp.hidden_size, hp.pta_dropout_rate)\n",
    "        decoder = model.Decoder(150, hp.emb_size, hp.pta_dropout_rate)\n",
    "        print(decoder)\n",
    "        encoder_optim = make_optim(encoder,\n",
    "                                     hp.pta_optimizer,\n",
    "                                     hp.pta_learning_rate,\n",
    "                                     hp.pta_lr_decay,\n",
    "                                     hp.pta_max_grad_norm)\n",
    "        decoder_optim = make_optim(decoder,\n",
    "                                            hp.pta_optimizer,\n",
    "                                            hp.pta_learning_rate,\n",
    "                                            hp.pta_lr_decay,\n",
    "                                            hp.pta_max_grad_norm)\n",
    "        if hp.pre_data == 'random':\n",
    "            checkpoint = pre_train_autoencoder(hp,encoder, encoder_optim,decoder, decoder_optim, train_words, dev_words, word2emb,shuffle_data1)\n",
    "        elif hp.pre_data == 'common':\n",
    "            checkpoint = pre_train_autoencoder(hp,\n",
    "                                          encoder,\n",
    "                                          encoder_optim,\n",
    "                                          decoder,\n",
    "                                          decoder_optim,\n",
    "                                          emb,\n",
    "                                          dev_words=dev_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "def save_checkpoint(hp, mode, model):\n",
    "    model_state_dict = model.state_dict()\n",
    "    checkpoint = {\n",
    "        mode: model_state_dict,\n",
    "        'hp': hp\n",
    "    }\n",
    "    torch.save(checkpoint,\n",
    "        '{}{}_checkpoint'.format('./', mode))\n",
    "\n",
    "def pre_train_classifier(hp,\n",
    "                         female_classifier,\n",
    "                         male_classifier,\n",
    "                         female_optim,\n",
    "                         male_optim,\n",
    "                         train_female_embs,\n",
    "                         train_male_embs,\n",
    "                         train_stereotype_embs,\n",
    "                         dev_female_embs,\n",
    "                         dev_male_embs,\n",
    "                         dev_stereotype_embs):\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    def run_model(female_embs, male_embs, no_gender_embs, mode):\n",
    "        inputs = female_embs + male_embs + no_gender_embs\n",
    "        tags = [0 for _ in range(len(female_embs))] + [1 for _ in range(len(male_embs))] + [2 for _ in range(len(no_gender_embs))]\n",
    "\n",
    "        if mode == 'train':\n",
    "            female_classifier.train()\n",
    "            male_classifier.train()\n",
    "            perm = [idx for idx in range(len(inputs))]\n",
    "            random.shuffle(perm)\n",
    "            inputs = [inputs[idx] for idx in perm]\n",
    "            tags = [tags[idx] for idx in perm]\n",
    "        elif mode == 'eval':\n",
    "            female_classifier.eval()\n",
    "            male_classifier.eval()\n",
    "        total_female_num = 0\n",
    "        total_male_num = 0\n",
    "        total_female_loss = 0\n",
    "        total_male_loss = 0\n",
    "        for input, tag in zip(inputs, tags):\n",
    "            zero_tag = torch.zeros(1)\n",
    "            one_tag = torch.ones(1)\n",
    "            female_classifier.zero_grad()\n",
    "            male_classifier.zero_grad()\n",
    "            female_pre = female_classifier(input)\n",
    "            male_pre = male_classifier(input)\n",
    "            if tag == 0:\n",
    "                female_loss = criterion(female_pre, one_tag)\n",
    "                male_loss = criterion(male_pre, zero_tag)\n",
    "            elif tag == 1:\n",
    "                female_loss = criterion(female_pre, zero_tag)\n",
    "                male_loss = criterion(male_pre, one_tag)\n",
    "            elif tag == 2:\n",
    "                female_loss = criterion(female_pre, zero_tag)\n",
    "                male_loss = criterion(male_pre, zero_tag)\n",
    "                #continue\n",
    "            if mode == 'train':\n",
    "                female_loss.backward()\n",
    "                male_loss.backward()\n",
    "                female_optim.step()\n",
    "                male_optim.step()\n",
    "            total_female_loss += female_loss.item()\n",
    "            total_male_loss += male_loss.item()\n",
    "            total_female_num += len(female_embs)\n",
    "            total_male_num += len(male_embs)\n",
    "\n",
    "        return total_female_loss / total_female_num, total_male_loss / total_male_num\n",
    "\n",
    "\n",
    "    min_female_loss = float('inf')\n",
    "    min_male_loss = float('inf')\n",
    "    for epoch in range(1, 100):\n",
    "        train_female_loss, train_male_loss = run_model(train_female_embs, train_male_embs, train_stereotype_embs, 'train')\n",
    "        eval_female_loss, eval_male_loss = run_model(dev_female_embs, dev_male_embs, dev_stereotype_embs, 'eval')\n",
    "\n",
    "        if eval_female_loss < min_female_loss:\n",
    "            min_female_epoch = epoch\n",
    "            min_female_loss = eval_female_loss\n",
    "            female_state_dict = female_classifier.state_dict()\n",
    "            female_checkpoint = {\n",
    "                'female': female_state_dict,\n",
    "                'hp': hp\n",
    "            }\n",
    "            torch.save(female_checkpoint,\n",
    "                '{}female_checkpoint'.format('./'))\n",
    "        if eval_male_loss < min_male_loss:\n",
    "            min_male_epoch = epoch\n",
    "            min_male_loss = eval_male_loss\n",
    "            male_state_dict = male_classifier.state_dict()\n",
    "            male_checkpoint = {\n",
    "                'male': male_state_dict,\n",
    "                'hp': hp\n",
    "            }\n",
    "            torch.save(male_checkpoint,\n",
    "                '{}male_checkpoint'.format('./'))\n",
    "\n",
    "    female_checkpoint = torch.load('{}female_checkpoint'.format('./'))\n",
    "    torch.save(female_checkpoint, '{}female.pt'.format('./'))\n",
    "    male_checkpoint = torch.load('{}male_checkpoint'.format('./'))\n",
    "    torch.save(male_checkpoint, '{}male.pt'.format('./'))\n",
    "\n",
    "    import os\n",
    "    os.remove('{}female_checkpoint'.format('./'))\n",
    "    os.remove('{}male_checkpoint'.format('./'))\n",
    "\n",
    "    return female_checkpoint, male_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=model.Encoder(hp.emb_size, hp.hidden_size, hp.dropout_rate)\n",
    "decoder =model.Decoder(hp.hidden_size//2, hp.emb_size, hp.dropout_rate)\n",
    "\n",
    "if hp.pre_train_autoencoder:\n",
    "    encoder.load_state_dict(checkpoint['encoder'])\n",
    "    decoder.load_state_dict(checkpoint['decoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training classifier\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if hp.pre_train_classifier:\n",
    "    female_classifier = model.Classifier(hp.hidden_size)\n",
    "    male_classifier = model.Classifier(hp.hidden_size)\n",
    "    female_classifier_optim = make_optim(female_classifier,\n",
    "                                 hp.cls_optimizer,\n",
    "                                 hp.cls_learning_rate,\n",
    "                                 hp.cls_lr_decay,\n",
    "                                 hp.cls_max_grad_norm)\n",
    "    male_classifier_optim = make_optim(male_classifier,\n",
    "                                        hp.cls_optimizer,\n",
    "                                        hp.cls_learning_rate,\n",
    "                                        hp.cls_lr_decay,\n",
    "                                        hp.cls_max_grad_norm)\n",
    "\n",
    "    encoder.eval()\n",
    "    encoder.zero_grad()\n",
    "\n",
    "    train_females = []\n",
    "    train_males = []\n",
    "    dev_females = []\n",
    "    dev_males = []\n",
    "\n",
    "    train_female_embs = [encoder(torch.FloatTensor(emb[word[0]])).data for word in train_words['female&male']]\n",
    "    encoder.zero_grad()\n",
    "    train_male_embs = [encoder(torch.FloatTensor(emb[word[1]])).data for word in train_words['female&male']]\n",
    "    encoder.zero_grad()\n",
    "    train_stereotype_embs = [encoder(torch.FloatTensor(emb[word])).data for word in train_words['no_gender']]\n",
    "    encoder.zero_grad()\n",
    "\n",
    "    dev_female_embs = [encoder(torch.FloatTensor(emb[word[0]])).data for word in dev_words['female&male']]\n",
    "    encoder.zero_grad()\n",
    "    dev_male_embs = [encoder(torch.FloatTensor(emb[word[1]])).data for word in dev_words['female&male']]\n",
    "    encoder.zero_grad()\n",
    "    dev_stereotype_embs = [encoder(torch.FloatTensor(emb[word])).data for word in dev_words['no_gender']]\n",
    "    encoder.zero_grad()\n",
    "\n",
    "    print('Pre-training classifier')\n",
    "    female_checkpoint, male_checkpoint = pre_train_classifier(hp,\n",
    "                                        female_classifier,\n",
    "                                        male_classifier,\n",
    "                                        female_classifier_optim,\n",
    "                                        male_classifier_optim,\n",
    "                                        train_female_embs,\n",
    "                                        train_male_embs,\n",
    "                                        train_stereotype_embs,\n",
    "                                        dev_female_embs,\n",
    "                                        dev_male_embs,\n",
    "                                        dev_stereotype_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building female & male classifiers\n",
      "Setting optimizer\n"
     ]
    }
   ],
   "source": [
    "print('Building female & male classifiers')\n",
    "female_classifier = model.Classifier(hp.hidden_size)\n",
    "male_classifier = model.Classifier(hp.hidden_size)\n",
    "\n",
    "if hp.pre_train_classifier:\n",
    "    female_classifier.load_state_dict(female_checkpoint['female'])\n",
    "    male_classifier.load_state_dict(male_checkpoint['male'])\n",
    "\n",
    "print('Setting optimizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building female & male classifiers\n",
      "setting optimizer\n"
     ]
    }
   ],
   "source": [
    "encoder=model.Encoder(hp.emb_size, hp.hidden_size, hp.dropout_rate)\n",
    "decoder =model.Decoder(hp.hidden_size//2, hp.emb_size, hp.dropout_rate)\n",
    "\n",
    "\n",
    "print('Building female & male classifiers')\n",
    "female_classifier = model.Classifier(hp.hidden_size)\n",
    "male_classifier = model.Classifier(hp.hidden_size)\n",
    "\n",
    "\n",
    "print(\"setting optimizer\")\n",
    "encoder_optim = make_optim(encoder,\n",
    "                                 hp.optimizer,\n",
    "                                 hp.learning_rate,\n",
    "                                 hp.lr_decay,\n",
    "                                 hp.max_grad_norm)\n",
    "female_classifier_optim = make_optim(female_classifier,\n",
    "                                    hp.optimizer,\n",
    "                                    hp.learning_rate,\n",
    "                                    hp.lr_decay,\n",
    "                                    hp.max_grad_norm)\n",
    "male_classifier_optim = make_optim(male_classifier,\n",
    "                                    hp.optimizer,\n",
    "                                    hp.learning_rate,\n",
    "                                    hp.lr_decay,\n",
    "                                    hp.max_grad_norm)\n",
    "decoder_optim = make_optim(decoder,\n",
    "                                    hp.optimizer,\n",
    "                                    hp.learning_rate,\n",
    "                                    hp.lr_decay,\n",
    "                                    hp.max_grad_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_function(mu, logsigma, kl_weight=0.0001):\n",
    "  \n",
    "    latent_loss = 0.5*torch.sum(logsigma.exp() + mu.pow(2)-1-logsigma)\n",
    "    vae_loss = kl_weight*latent_loss \n",
    "    #print(vae_loss)\n",
    "    return vae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debiasing_loss_function(mu, logsigma):\n",
    "\n",
    "    vae_loss = vae_loss_function(mu,logsigma) # TODO\n",
    "\n",
    "    total_loss = tf.reduce_mean(vae_loss)\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(z_mean,z_logsigma):\n",
    "    #print(hp.hid.shape)\n",
    "    #print(z_logsigma)\n",
    "    batch, latent_dim = z_mean.shape\n",
    "    epsilon = torch.rand((batch, latent_dim))\n",
    "    \n",
    "    z = z_mean + torch.exp(0.5*z_logsigma)*epsilon\n",
    "    #print(z.shape)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n"
     ]
    }
   ],
   "source": [
    "train(encoder, encoder_optim, female_classifier, female_classifier_optim,male_classifier, male_classifier_optim,\n",
    "           decoder, decoder_optim, train_words, dev_words, word2emb,shuffle_data1,sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, encoder_optim, female_classifier, female_classifier_optim,male_classifier, male_classifier_optim,\n",
    "           decoder, decoder_optim, train_words, dev_words, word2emb,shuffle_data1,sampling):\n",
    "    \n",
    "    train_size  = len(train_words['no_gender']) + len(train_words['female&male'])+ len(train_words['stereotype'])\n",
    "    dev_size = len(dev_words['no_gender']) + len(dev_words['female&male']) +len(dev_words['stereotype'])\n",
    "    classifier_criterion = nn.BCELoss()\n",
    "    decoder_criterion = nn.MSELoss()\n",
    "    encoder1=encoder\n",
    "    decode2 =decoder\n",
    "    def encode(x):\n",
    "        #encoder.zero_grad()\n",
    "        \n",
    "        for i,words in enumerate(x):\n",
    "            emb1 =  torch.stack([torch.from_numpy(word2emb[word]) for word in words])\n",
    "            encoder_output = encoder(emb1)\n",
    "        z_mean = encoder_output[:,:hp.hidden_size//2]\n",
    "       \n",
    "        z_logsigma = encoder_output[:,hp.hidden_size//2:]\n",
    "        \n",
    "        return encoder_output, z_mean, z_logsigma\n",
    "    def encoder_1(emb_1):\n",
    "        encoder_output = encoder(emb_1)\n",
    "        z_mean = encoder_output[:,:hp.hidden_size//2]\n",
    "       \n",
    "        z_logsigma = encoder_output[:,hp.hidden_size//2:]\n",
    "        \n",
    "        return encoder_output, z_mean, z_logsigma\n",
    "    \n",
    "    def get_latent_mu(words,batch_size=4):\n",
    "        \n",
    "        N = len(words)\n",
    "        mu = np.zeros((N, hp.hidden_size//2))\n",
    "        for start_ind in range(0, N, batch_size):\n",
    "            end_ind = min(start_ind+batch_size, N+1)\n",
    "            if end_ind%batch_size!=0:\n",
    "                break\n",
    "            batch = (words[start_ind:end_ind])\n",
    "            _,batch_mu, _ = encode(batch)\n",
    "            \n",
    "            mu[start_ind:end_ind] = batch_mu.detach().numpy()\n",
    "        return mu\n",
    "    def get_training_sample_probabilities(train_words,bins=4, smoothing_fac=0.005):\n",
    "        mu = get_latent_mu(train_words)\n",
    "        training_sample_p = np.zeros(mu.shape[0])\n",
    "        for i in range(hp.hidden_size//2):\n",
    "            latent_distribution = mu[:,i]\n",
    "            # generate a histogram of the latent distribution\n",
    "            hist_density, bin_edges =  np.histogram(latent_distribution, density=True, bins=bins)\n",
    "\n",
    "            # find which latent bin every data sample falls in \n",
    "            bin_edges[0] = -float('inf')\n",
    "            bin_edges[-1] = float('inf')\n",
    "\n",
    "            #call the digitize function to find which bins in the latent distribution every data sample falls in to\n",
    "            # https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.digitize.html\n",
    "            bin_idx = np.digitize(latent_distribution, bin_edges) \n",
    "\n",
    "            # smooth the density function\n",
    "            hist_smoothed_density = hist_density + smoothing_fac\n",
    "            hist_smoothed_density = hist_smoothed_density / np.sum(hist_smoothed_density)\n",
    "\n",
    "            # invert the density function \n",
    "            p = 1.0/(hist_smoothed_density[bin_idx-1])\n",
    "\n",
    "            # TODO: normalize all probabilities\n",
    "            p = p/np.sum(p)\n",
    "\n",
    "            # update sampling probabilities by considering whether the newly\n",
    "            #     computed p is greater than the existing sampling probabilities.\n",
    "            training_sample_p = np.maximum(p, training_sample_p)\n",
    "        \n",
    "    # final normalization\n",
    "        training_sample_p /= np.sum(training_sample_p)\n",
    "\n",
    "        return training_sample_p\n",
    "    def run_model(words, gender_vektor, mode,p_in):\n",
    "        #batch_size=4\n",
    "        if mode==\"train\":\n",
    "            encoder.train()\n",
    "            female_classifier.train()\n",
    "            male_classifier.train()\n",
    "            decoder.train()\n",
    "        elif model == 'eval':\n",
    "            encoder.eval()\n",
    "            female_classifier.eval()\n",
    "            male_classifier.eval()\n",
    "            decoder.eval()\n",
    "        total_loss = 0\n",
    "        total_female_classifier_loss = 0\n",
    "        total_male_classifier_loss = 0\n",
    "        total_decoder_loss = 0\n",
    "        total_gender_stereotype_loss = 0\n",
    "        total_gender_no_gender_loss = 0\n",
    "        total_gender_vektor_loss = 0\n",
    "        total_vae_loss = 0\n",
    "        female_classifier_correct = 0\n",
    "        male_classifier_correct = 0\n",
    "        vae_loss=0\n",
    "        if mode=='train':\n",
    "            limit_size =  max([len(words['no_gender']), len(words['female&male']),\n",
    "                               len(words['stereotype'])])\n",
    "            words['no gender'],_ = \\\n",
    "                shuffle_data(words['no_gender'],\n",
    "                             limit_size=limit_size,\n",
    "                             sampling = hp.sampling)\n",
    "            words['female & male'],_ = \\\n",
    "                shuffle_data(words['female&male'],\n",
    "                             limit_size=limit_size,\n",
    "                             sampling=hp.sampling)\n",
    "            words['stereotype'],_ = \\\n",
    "                shuffle_data(words['stereotype'],\n",
    "                             limit_size=limit_size,\n",
    "                             sampling=hp.sampling)\n",
    "        elif mode=='eval':\n",
    "            limit_size = hp.dev_num\n",
    "        \n",
    "        inputs = [[gender[0], gender[1], stereotype, no_gender] for gender, stereotype, no_gender in zip(\n",
    "            words['female&male'], words['stereotype'], words['no_gender'])]\n",
    "        if mode=='eval':\n",
    "            x=inputs\n",
    "        else:\n",
    "            x = shuffle_data1(inputs,p_in)\n",
    "        \n",
    "        data_size = len(x)*4\n",
    "        def classify(hidden, gold, classifier):\n",
    "            classifier.zero_grad()\n",
    "            pre = classifier(hidden)\n",
    "            loss = classifier_criterion(pre,gold)\n",
    "            return pre, loss\n",
    "        b_s = 4\n",
    "        for j in range(len(inputs)//4):\n",
    "            if mode==\"train\":\n",
    "                y=x[:32]\n",
    "            if mode=='eval':\n",
    "                #j=4\n",
    "                y=inputs\n",
    "            for i,words in enumerate(y):\n",
    "                #print(y)\n",
    "                #z_mean,z_logsigma,recon = call(batch)\n",
    "                #vae_loss_1  = debiasing_loss_function(z_mean,z_logsigma)\n",
    "                emb_dummy =  torch.stack([torch.from_numpy(word2emb[word]) for word in words])\n",
    "                encoder.zero_grad()\n",
    "                hidden, z_mean,z_logsigma = encoder_1(emb_dummy)\n",
    "                z = sampling(z_mean,z_logsigma)\n",
    "                decoder.zero_grad()\n",
    "                pre = decoder(z)\n",
    "                vae_loss = vae_loss_function(z_mean,z_logsigma)\n",
    "                decoder_loss = decoder_criterion(pre, emb_dummy)\n",
    "                \n",
    "                female_gold = torch.FloatTensor([1,0,0,0]).view(-1,1)\n",
    "                male_gold = torch.FloatTensor([0,1,0,0]).view(-1,1)\n",
    "                female_pre, female_loss = classify(hidden, female_gold, female_classifier)\n",
    "                \n",
    "                male_pre, male_loss = classify(hidden, male_gold, male_classifier)\n",
    "                gender_stereotype_loss = torch.sum(gender_vektor*hidden[2])**2\n",
    "                gender_no_gender_loss = torch.sum(gender_vektor*hidden[3])**2\n",
    "                gender_vektor_loss  = decoder_criterion(gender_vektor,(hidden[1]-hidden[0]))\n",
    "                decoder_loss *=hp.decoder_loss_rate\n",
    "                female_loss*=hp.female_loss_rate\n",
    "                male_loss *=hp.male_loss_rate\n",
    "                gender_stereotype_loss*=hp.gender_stereotype_loss_rate\n",
    "                gender_no_gender_loss*=hp.gender_no_gender_loss_rate\n",
    "                gender_vektor_loss *=hp.gender_vektor_loss_rate\n",
    "                #vae_loss *= (0.0005)\n",
    "                loss = decoder_loss+female_loss+male_loss+gender_stereotype_loss+gender_no_gender_loss+gender_vektor_loss+vae_loss\n",
    "                #print(loss)\n",
    "                total_decoder_loss+=decoder_loss.item()\n",
    "                total_gender_stereotype_loss+=gender_stereotype_loss.item()\n",
    "                total_female_classifier_loss += female_loss.item()\n",
    "                total_male_classifier_loss += male_loss.item()\n",
    "                total_gender_no_gender_loss+=gender_no_gender_loss.item()\n",
    "                total_gender_vektor_loss+=gender_vektor_loss.item()\n",
    "                total_vae_loss+=vae_loss.item()\n",
    "                #total_vae_loss+=vae_loss_1.item()\n",
    "                total_loss+=loss.item()\n",
    "            \n",
    "                female_classifier_correct +=torch.sum(torch.eq(female_gold,(female_pre>0.5).float().view(-1))).item()\n",
    "                male_classifier_correct+=torch.sum(torch.eq(male_gold,(male_pre>0.5).float().view(-1))).item()\n",
    "                if mode=='train':\n",
    "                    loss.backward()\n",
    "                    encoder_optim.step()\n",
    "                    female_classifier_optim.step()\n",
    "                    male_classifier_optim.step()\n",
    "                    decoder_optim.step()\n",
    "                \n",
    "        return total_decoder_loss/data_size, total_female_classifier_loss/data_size,total_male_classifier_loss/data_size,total_gender_stereotype_loss / data_size, total_gender_no_gender_loss / data_size,total_gender_vektor_loss / data_size,total_loss / data_size,female_classifier_correct / data_size, male_classifier_correct / data_size,total_vae_loss/data_size\n",
    "\n",
    "    def calculate_gender_vektor(encoder, word2emb, gender_pairs):\n",
    "        #encoder.eval()\n",
    "        #encoder.zero_grad()\n",
    "        females=[]\n",
    "        males=[]\n",
    "        for female, male in gender_pairs:\n",
    "            females+=[female]\n",
    "            males+=[male]\n",
    "        #print(gender_pairs)\n",
    "        female_embs = torch.stack([encoder(torch.FloatTensor(emb[word])).data for word in females], dim=0)\n",
    "        male_embs = torch.stack([encoder(torch.FloatTensor(emb[word])).data for word in males], dim=0)\n",
    "        gender_vektor = torch.sum(male_embs-female_embs,0)/male_embs.size(0)\n",
    "        return gender_vektor\n",
    "    \n",
    "    print('start training')\n",
    "    best_loss = float('inf')\n",
    "    decoder_loss_list=[]\n",
    "    female_classifier_loss_list=[]\n",
    "    male_classifier_loss_list=[]\n",
    "    gender_stereotype_loss_list = []\n",
    "    gender_no_gender_loss_list = []\n",
    "    gender_vektor_loss_list = []\n",
    "    total_loss_list = []\n",
    "    female_classifier_acc_list = []\n",
    "    male_classifier_acc_list = []\n",
    "    vae_loss_list=[]\n",
    "    for epochs in range(5):\n",
    "        logs={}\n",
    "        inputs1 = [[gender[0], gender[1], stereotype, no_gender] for gender, stereotype, no_gender in zip(\n",
    "            train_words['female&male'], train_words['stereotype'], train_words['no_gender'])]\n",
    "        p_inp = get_training_sample_probabilities(inputs1)\n",
    "        gender_vektor = calculate_gender_vektor(encoder,emb, train_words['female&male'])\n",
    "        #l1=[]\n",
    "        for j in range(25):\n",
    "            results = run_model(copy.deepcopy(train_words), gender_vektor, mode='train', p_in=p_inp)\n",
    "            decoder_loss_list += [results[0]]\n",
    "            female_classifier_loss_list += [results[1]]\n",
    "            male_classifier_loss_list += [results[2]]\n",
    "            gender_stereotype_loss_list += [results[3]]\n",
    "            if hp.gender_no_gender_loss:\n",
    "                gender_no_gender_loss_list += [results[4]]\n",
    "            if hp.gender_vektor_loss:\n",
    "                gender_vektor_loss_list += [results[5]]\n",
    "            total_loss_list += [results[6]]\n",
    "            \n",
    "            female_classifier_acc_list += [results[7]]\n",
    "            male_classifier_acc_list += [results[8]]\n",
    "            vae_loss_list+=[results[9]]\n",
    "            \n",
    "            #liveloss.update(logs)\n",
    "            #liveloss.send()\n",
    "        results = run_model(copy.deepcopy(dev_words), gender_vektor, mode='eval',p_in=p_inp)\n",
    "        \n",
    "        decoder_loss_list += [results[0]]\n",
    "        female_classifier_loss_list += [results[1]]\n",
    "        male_classifier_loss_list += [results[2]]\n",
    "        gender_stereotype_loss_list += [results[3]]\n",
    "        if hp.gender_no_gender_loss:\n",
    "            gender_no_gender_loss_list += [results[4]]\n",
    "        if hp.gender_vektor_loss:\n",
    "            gender_vektor_loss_list += [results[5]]\n",
    "        total_loss = results[6]\n",
    "        #print(\"eval_loss\",total_loss)\n",
    "        total_loss_list += [total_loss]\n",
    "        female_classifier_acc_list += [results[7]]\n",
    "        male_classifier_acc_list += [results[8]]\n",
    "        vae_loss_list+=[results[9]]\n",
    "        if total_loss < best_loss:\n",
    "            best_epoch = epochs\n",
    "            best_loss = total_loss\n",
    "            encoder_state_dict = encoder.state_dict()\n",
    "            checkpoint = {\n",
    "                'encoder': encoder_state_dict,\n",
    "                'hp': hp,\n",
    "                'encoder_optim': encoder_optim\n",
    "            }\n",
    "            torch.save(checkpoint, '{}model_checkpoint'.format('./'))\n",
    "    checkpoint = torch.load('{}model_checkpoint'.format('./'))\n",
    "    torch.save(checkpoint,\n",
    "               '{}best_model.pt'.format('./', best_loss, best_epoch))\n",
    "    #loss_history.append(total_loss_list.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
